{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output of the docling document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements and first conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook environment setup complete\n"
     ]
    }
   ],
   "source": [
    "from notebook_utils import setup_notebook_environment, style_dataframe\n",
    "setup_notebook_environment()\n",
    "\n",
    "from docling.chunking import HierarchicalChunker, HybridChunker\n",
    "from docling.document_converter import DocumentConverter\n",
    "from app.utils.tokenizer import OpenAITokenizerWrapper\n",
    "from app.config.settings import settings\n",
    "from pathlib import Path\n",
    "\n",
    "from pathlib import Path\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "# from app.services.document_service import DocumentService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sng/nanobot-poc/data/converted_docs')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_docs_path = settings.file_paths.get_converted_docs_path()\n",
    "converted_docs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RESOLUTION_SCALE = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_doc_path = 'https://docs.google.com/document/d/1YGpHp7avHQMojJRbkAyP8io0ZqarRvMKm_ulQ6dYq0g/export?format=pdf'\n",
    "output_dir = Path(converted_docs_path)\n",
    "\n",
    "# Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n",
    "# will destroy them for cleaning up memory.\n",
    "# This is done by setting PdfPipelineOptions.images_scale, which also defines the scale of images.\n",
    "# scale=1 correspond of a standard 72 DPI image\n",
    "# The PdfPipelineOptions.generate_* are the selectors for the document elements which will be enriched\n",
    "# with the image field\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "conv_res = doc_converter.convert(input_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "doc_filename = 'test_sop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save page images\n",
    "for page_no, page in conv_res.document.pages.items():\n",
    "    page_no = page.page_no\n",
    "    page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n",
    "    with page_image_filename.open(\"wb\") as fp:\n",
    "        page.image.pil_image.save(fp, format=\"PNG\")\n",
    "\n",
    "# Save images of figures and tables\n",
    "table_counter = 0\n",
    "picture_counter = 0\n",
    "for element, _level in conv_res.document.iterate_items():\n",
    "    if isinstance(element, TableItem):\n",
    "        table_counter += 1\n",
    "        element_image_filename = (\n",
    "            output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "        )\n",
    "        with element_image_filename.open(\"wb\") as fp:\n",
    "            element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "    if isinstance(element, PictureItem):\n",
    "        picture_counter += 1\n",
    "        element_image_filename = (\n",
    "            output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "        )\n",
    "        with element_image_filename.open(\"wb\") as fp:\n",
    "            element.get_image(conv_res.document).save(fp, \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save markdown with embedded pictures\n",
    "md_filename = output_dir / f\"{doc_filename}-with-images.md\"\n",
    "conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)\n",
    "\n",
    "# Save markdown with externally referenced pictures\n",
    "md_filename = output_dir / f\"{doc_filename}-with-image-refs.md\"\n",
    "conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "# Save HTML with externally referenced pictures\n",
    "html_filename = output_dir / f\"{doc_filename}-with-image-refs.html\"\n",
    "conv_res.document.save_as_html(html_filename, image_mode=ImageRefMode.REFERENCED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = 'https://docs.google.com/document/d/1YGpHp7avHQMojJRbkAyP8io0ZqarRvMKm_ulQ6dYq0g/export?format=pdf'\n",
    "    output_dir = Path(\"scratch\")\n",
    "\n",
    "    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n",
    "    # will destroy them for cleaning up memory.\n",
    "    # This is done by setting PdfPipelineOptions.images_scale, which also defines the scale of images.\n",
    "    # scale=1 correspond of a standard 72 DPI image\n",
    "    # The PdfPipelineOptions.generate_* are the selectors for the document elements which will be enriched\n",
    "    # with the image field\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc_filename = conv_res.input.file.stem\n",
    "\n",
    "    # Save page images\n",
    "    for page_no, page in conv_res.document.pages.items():\n",
    "        page_no = page.page_no\n",
    "        page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n",
    "        with page_image_filename.open(\"wb\") as fp:\n",
    "            page.image.pil_image.save(fp, format=\"PNG\")\n",
    "\n",
    "    # Save images of figures and tables\n",
    "    table_counter = 0\n",
    "    picture_counter = 0\n",
    "    for element, _level in conv_res.document.iterate_items():\n",
    "        if isinstance(element, TableItem):\n",
    "            table_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "        if isinstance(element, PictureItem):\n",
    "            picture_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "    # Save markdown with embedded pictures\n",
    "    md_filename = output_dir / f\"{doc_filename}-with-images.md\"\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)\n",
    "\n",
    "    # Save markdown with externally referenced pictures\n",
    "    md_filename = output_dir / f\"{doc_filename}-with-image-refs.md\"\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "    # Save HTML with externally referenced pictures\n",
    "    html_filename = output_dir / f\"{doc_filename}-with-image-refs.html\"\n",
    "    conv_res.document.save_as_html(html_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    _log.info(f\"Document converted and figures exported in {end_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for DocumentConverter.convert\ninput_format\n  Unexpected keyword argument [type=unexpected_keyword_argument, input_value=<InputFormat.PDF: 'pdf'>, input_type=InputFormat]\n    For further information visit https://errors.pydantic.dev/2.10/v/unexpected_keyword_argument\npipeline_options\n  Unexpected keyword argument [type=unexpected_keyword_argument, input_value=PdfPipelineOptions(create...rate_table_images=False), input_type=PdfPipelineOptions]\n    For further information visit https://errors.pydantic.dev/2.10/v/unexpected_keyword_argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m doc_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://docs.google.com/document/d/1YGpHp7avHQMojJRbkAyP8io0ZqarRvMKm_ulQ6dYq0g/export?format=pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Convert the document\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInputFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPDF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_options\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanobot-poc/.venv/lib/python3.12/site-packages/pydantic/_internal/_validate_call.py:38\u001b[0m, in \u001b[0;36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(wrapped)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nanobot-poc/.venv/lib/python3.12/site-packages/pydantic/_internal/_validate_call.py:111\u001b[0m, in \u001b[0;36mValidateCallWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 111\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydantic_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__(res)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for DocumentConverter.convert\ninput_format\n  Unexpected keyword argument [type=unexpected_keyword_argument, input_value=<InputFormat.PDF: 'pdf'>, input_type=InputFormat]\n    For further information visit https://errors.pydantic.dev/2.10/v/unexpected_keyword_argument\npipeline_options\n  Unexpected keyword argument [type=unexpected_keyword_argument, input_value=PdfPipelineOptions(create...rate_table_images=False), input_type=PdfPipelineOptions]\n    For further information visit https://errors.pydantic.dev/2.10/v/unexpected_keyword_argument"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize pipeline options\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.images_scale = 2.0  # Adjust resolution as needed\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "# Create the document converter\n",
    "converter = DocumentConverter()\n",
    "doc_path = 'https://docs.google.com/document/d/1YGpHp7avHQMojJRbkAyP8io0ZqarRvMKm_ulQ6dYq0g/export?format=pdf'\n",
    "\n",
    "# Convert the document\n",
    "document = converter.convert(\n",
    "    doc_path,\n",
    "    input_format=InputFormat.PDF,\n",
    "    pipeline_options=pipeline_options\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize pipeline options\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.images_scale = 2.0  # Adjust resolution as needed\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "# Create the document converter\n",
    "converter = DocumentConverter()\n",
    "doc_path = 'https://docs.google.com/document/d/1YGpHp7avHQMojJRbkAyP8io0ZqarRvMKm_ulQ6dYq0g/export?format=pdf'\n",
    "\n",
    "# Convert the document\n",
    "document = converter.convert(\n",
    "    doc_path,\n",
    "    input_format=InputFormat.PDF,\n",
    "    pipeline_options=pipeline_options\n",
    ")\n",
    "\n",
    "# Get the base output directory\n",
    "converted_docs_path = settings.file_paths.get_converted_docs_path()\n",
    "base_name = \"pdf_document\"  # or whatever name you want\n",
    "output_dir = converted_docs_path / base_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save markdown with referenced images\n",
    "md_path = output_dir / f\"{base_name}.md\"\n",
    "document.save_as_markdown(md_path, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "# Save images and tables\n",
    "image_counter = 0\n",
    "table_counter = 0\n",
    "\n",
    "# Iterate through all elements in the document\n",
    "for element, _level in document.iterate_items():\n",
    "    if isinstance(element, PictureItem):\n",
    "        image_counter += 1\n",
    "        # Save image\n",
    "        image_path = output_dir / f\"image_{image_counter}.png\"\n",
    "        image = element.get_image(document)\n",
    "        if image is not None:\n",
    "            with image_path.open(\"wb\") as fp:\n",
    "                image.save(fp, \"PNG\")\n",
    "            print(f\"Saved image to: {image_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Could not get image for element {image_counter}\")\n",
    "        \n",
    "    elif isinstance(element, TableItem):\n",
    "        table_counter += 1\n",
    "        # Save table as image\n",
    "        table_path = output_dir / f\"table_{table_counter}.png\"\n",
    "        table_image = element.get_image(document)\n",
    "        if table_image is not None:\n",
    "            with table_path.open(\"wb\") as fp:\n",
    "                table_image.save(fp, \"PNG\")\n",
    "            print(f\"Saved table to: {table_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Could not get table image for element {table_counter}\")\n",
    "\n",
    "print(f\"Saved markdown to: {md_path}\")\n",
    "print(f\"Total images saved: {image_counter}\")\n",
    "print(f\"Total tables saved: {table_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from app.config.settings import settings\n",
    "\n",
    "# Initialize pipeline options\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.images_scale = 2.0  # Adjust resolution as needed\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "# Create the document converter\n",
    "converter = DocumentConverter()\n",
    "doc_path = 'https://docs.google.com/document/d/1YGpHp7avHQMojJRbkAyP8io0ZqarRvMKm_ulQ6dYq0g/export?format=pdf'\n",
    "\n",
    "# Convert the document\n",
    "document = converter.convert(\n",
    "    doc_path,\n",
    "    input_format=InputFormat.PDF,\n",
    "    pipeline_options=pipeline_options\n",
    ")\n",
    "\n",
    "# Get the base output directory\n",
    "converted_docs_path = settings.file_paths.get_converted_docs_path()\n",
    "base_name = \"pdf_document\"  # or whatever name you want\n",
    "output_dir = converted_docs_path / base_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save markdown with referenced images\n",
    "md_path = output_dir / f\"{base_name}.md\"\n",
    "document.save_as_markdown(md_path, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "# Save images and tables\n",
    "image_counter = 0\n",
    "table_counter = 0\n",
    "\n",
    "# Iterate through all elements in the document\n",
    "for element, _level in document.iterate_items():\n",
    "    if isinstance(element, PictureItem):\n",
    "        image_counter += 1\n",
    "        # Save image\n",
    "        image_path = output_dir / f\"image_{image_counter}.png\"\n",
    "        image = element.get_image(document)\n",
    "        if image is not None:\n",
    "            with image_path.open(\"wb\") as fp:\n",
    "                image.save(fp, \"PNG\")\n",
    "            print(f\"Saved image to: {image_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Could not get image for element {image_counter}\")\n",
    "        \n",
    "    elif isinstance(element, TableItem):\n",
    "        table_counter += 1\n",
    "        # Save table as image\n",
    "        table_path = output_dir / f\"table_{table_counter}.png\"\n",
    "        table_image = element.get_image(document)\n",
    "        if table_image is not None:\n",
    "            with table_path.open(\"wb\") as fp:\n",
    "                table_image.save(fp, \"PNG\")\n",
    "            print(f\"Saved table to: {table_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Could not get table image for element {table_counter}\")\n",
    "\n",
    "print(f\"Saved markdown to: {md_path}\")\n",
    "print(f\"Total images saved: {image_counter}\")\n",
    "print(f\"Total tables saved: {table_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = DocumentService()\n",
    "doc_path = 'https://docs.google.com/document/d/1YGpHp7avHQMojJRbkAyP8io0ZqarRvMKm_ulQ6dYq0g/export?format=pdf'\n",
    "result = converter.convert_url(doc_path)\n",
    "document = result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = DocumentConverter()\n",
    "doc_path = \"/home/sng/nanobot-poc/data/test/grant_decision_email_single_page.pdf\"\n",
    "result = converter.convert(doc_path)\n",
    "document = result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_data = document.export_to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"pdf_document.md\"  # or whatever name you want\n",
    "\n",
    "# Create the full path\n",
    "output_path = converted_docs_path / filename\n",
    "\n",
    "# Save the markdown data\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(md_data)\n",
    "\n",
    "print(f\"Saved markdown to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = DocumentService()\n",
    "doc_path = 'https://docs.google.com/document/d/1YGpHp7avHQMojJRbkAyP8io0ZqarRvMKm_ulQ6dYq0g/export?format=pdf'\n",
    "result = converter.convert_document(doc_path)\n",
    "document = result.document\n",
    "md_data = document.export_to_markdown()\n",
    "converted_docs_path = settings.file_paths.get_converted_docs_path()\n",
    "filename = \"pdf_document.md\"  # or whatever name you want\n",
    "# Create the full path\n",
    "output_path = converted_docs_path / filename\n",
    "# Save the markdown data\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(md_data)\n",
    "print(f\"Saved markdown to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from app.config.settings import settings\n",
    "\n",
    "# Initialize pipeline options\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.images_scale = 2.0  # Adjust resolution as needed\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "# Create the document converter\n",
    "converter = DocumentConverter()\n",
    "doc_path = 'https://docs.google.com/document/d/1YGpHp7avHQMojJRbkAyP8io0ZqarRvMKm_ulQ6dYq0g/export?format=pdf'\n",
    "\n",
    "# Convert the document\n",
    "document = converter.convert(\n",
    "    doc_path,\n",
    "    input_format=InputFormat.PDF,\n",
    "    pipeline_options=pipeline_options\n",
    ")\n",
    "\n",
    "# Get the base output directory\n",
    "converted_docs_path = settings.file_paths.get_converted_docs_path()\n",
    "base_name = \"pdf_document\"  # or whatever name you want\n",
    "output_dir = converted_docs_path / base_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save markdown with referenced images\n",
    "md_path = output_dir / f\"{base_name}.md\"\n",
    "document.save_as_markdown(md_path, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "# Save images and tables\n",
    "image_counter = 0\n",
    "table_counter = 0\n",
    "\n",
    "# Iterate through all elements in the document\n",
    "for element, _level in document.iterate_items():\n",
    "    if isinstance(element, PictureItem):\n",
    "        image_counter += 1\n",
    "        # Save image\n",
    "        image_path = output_dir / f\"image_{image_counter}.png\"\n",
    "        image = element.get_image(document)\n",
    "        if image is not None:\n",
    "            with image_path.open(\"wb\") as fp:\n",
    "                image.save(fp, \"PNG\")\n",
    "            print(f\"Saved image to: {image_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Could not get image for element {image_counter}\")\n",
    "        \n",
    "    elif isinstance(element, TableItem):\n",
    "        table_counter += 1\n",
    "        # Save table as image\n",
    "        table_path = output_dir / f\"table_{table_counter}.png\"\n",
    "        table_image = element.get_image(document)\n",
    "        if table_image is not None:\n",
    "            with table_path.open(\"wb\") as fp:\n",
    "                table_image.save(fp, \"PNG\")\n",
    "            print(f\"Saved table to: {table_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Could not get table image for element {table_counter}\")\n",
    "\n",
    "print(f\"Saved markdown to: {md_path}\")\n",
    "print(f\"Total images saved: {image_counter}\")\n",
    "print(f\"Total tables saved: {table_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.export_to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = OpenAITokenizerWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Document Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Document attributes (data):\")\n",
    "for attr in dir(document):\n",
    "    if not attr.startswith('_'):  # Skip private attributes\n",
    "        try:\n",
    "            value = getattr(document, attr)\n",
    "            if not callable(value):  # Only non-callable (attributes)\n",
    "                print(f\"- {attr}: {value}\")\n",
    "        except Exception as e:\n",
    "            print(f\"- {attr}: Error accessing ({str(e)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDocument methods (functions):\")\n",
    "for attr in dir(document):\n",
    "    if not attr.startswith('_'):  # Skip private attributes\n",
    "        try:\n",
    "            value = getattr(document, attr)\n",
    "            if callable(value):  # Only callable (methods)\n",
    "                print(f\"- {attr}()\")\n",
    "        except Exception as e:\n",
    "            print(f\"- {attr}: Error accessing ({str(e)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Text Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== TEXT ELEMENTS STRUCTURE ===\")\n",
    "for i, text_item in enumerate(document.texts):\n",
    "    print(f\"\\nText Element #{i+1}:\")\n",
    "    \n",
    "    # Get the type of the text element\n",
    "    element_type = type(text_item).__name__\n",
    "    print(f\"Type: {element_type}\")\n",
    "    \n",
    "    # Print text content with length information\n",
    "    text_length = len(text_item.text)\n",
    "    token_count = tokenizer.count_tokens(text_item.text)\n",
    "    print(f\"Text ({text_length} chars, {token_count} tokens): {text_item.text[:100]}...\" if text_length > 100 \n",
    "          else f\"Text ({text_length} chars, {token_count} tokens): {text_item.text}\")\n",
    "    \n",
    "    # Print other attributes based on the element type\n",
    "    if hasattr(text_item, 'label'):\n",
    "        print(f\"Label: {text_item.label}\")\n",
    "    \n",
    "    if hasattr(text_item, 'level') and element_type == 'SectionHeaderItem':\n",
    "        print(f\"Heading Level: {text_item.level}\")\n",
    "    \n",
    "    if hasattr(text_item, 'prov') and text_item.prov:\n",
    "        page_numbers = [prov.page_no for prov in text_item.prov]\n",
    "        print(f\"Page Numbers: {page_numbers}\")\n",
    "        \n",
    "        # Get bounding box information if available\n",
    "        if hasattr(text_item.prov[0], 'bbox'):\n",
    "            bbox = text_item.prov[0].bbox\n",
    "            print(f\"Bounding Box: left={bbox.l}, top={bbox.t}, right={bbox.r}, bottom={bbox.b}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the different types of elements\n",
    "element_types = {}\n",
    "for text_item in document.texts:\n",
    "    element_type = type(text_item).__name__\n",
    "    element_types[element_type] = element_types.get(element_type, 0) + 1\n",
    "\n",
    "print(\"\\n=== ELEMENT TYPE COUNTS ===\")\n",
    "for element_type, count in element_types.items():\n",
    "    print(f\"{element_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_items = [item for item in document.texts if type(item).__name__ == 'ListItem']\n",
    "print(f\"\\nNumber of list items found: {len(list_items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \"\"\n",
    "for text_item in document.texts:\n",
    "    full_text += text_item.text + \" \"\n",
    "print(f\"Total text length: {len(full_text)} characters\")\n",
    "print(f\"\\nFirst 200 chars of text: {full_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type annotation for document.texts\n",
    "import inspect\n",
    "from typing import get_type_hints\n",
    "\n",
    "# This might work depending on how Docling is implemented\n",
    "type_hints = get_type_hints(type(document))\n",
    "if 'texts' in type_hints:\n",
    "    print(f\"Type hint for texts: {type_hints['texts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_hints = get_type_hints(type(document))\n",
    "if 'texts' in type_hints:\n",
    "    type_hint_str = str(type_hints['texts'])\n",
    "    print(f\"Full type hint: {type_hint_str}\")\n",
    "    \n",
    "    # Extract just the class names\n",
    "    if 'Union[' in type_hint_str:\n",
    "        # Extract the part between Union[ and ]\n",
    "        union_content = type_hint_str.split('Union[')[1].split(']')[0]\n",
    "        \n",
    "        # Split by comma and extract class names\n",
    "        class_paths = [path.strip() for path in union_content.split(',')]\n",
    "        class_names = [path.split('.')[-1] for path in class_paths]\n",
    "        \n",
    "        print(\"\\nText element types:\")\n",
    "        for class_name in class_names:\n",
    "            print(f\"- {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for examining text element attributes\n",
    "print(\"\\n=== TEXT ELEMENT ATTRIBUTES ===\")\n",
    "for i, text_item in enumerate(document.texts):\n",
    "    print(f\"\\nText Element #{i+1}:\")\n",
    "    element_type = type(text_item).__name__\n",
    "    print(f\"Type: {element_type}\")\n",
    "    \n",
    "    # Show text preview\n",
    "    text_preview = text_item.text[:50] + \"...\" if len(text_item.text) > 50 else text_item.text\n",
    "    print(f\"Text: {text_preview}\")\n",
    "    \n",
    "    # Show list-specific attributes\n",
    "    if element_type == 'ListItem':\n",
    "        if hasattr(text_item, 'list_type'):\n",
    "            print(f\"List Type: {text_item.list_type}\")\n",
    "        if hasattr(text_item, 'list_index'):\n",
    "            print(f\"List Index: {text_item.list_index}\")\n",
    "    \n",
    "    # Show all other non-private attributes\n",
    "    print(\"Other attributes:\")\n",
    "    for attr in dir(text_item):\n",
    "        if not attr.startswith('_') and attr not in ['text', 'label', 'level', 'prov', 'list_type', 'list_index']:\n",
    "            try:\n",
    "                value = getattr(text_item, attr)\n",
    "                if not callable(value) and not isinstance(value, (list, dict)) and str(value) != '':\n",
    "                    print(f\"  - {attr}: {value}\")\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Furniture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect document furniture\n",
    "print(\"\\n=== DOCUMENT FURNITURE ===\")\n",
    "\n",
    "# Check if furniture exists\n",
    "if hasattr(document, 'furniture'):\n",
    "    furniture = document.furniture\n",
    "    \n",
    "    # Print basic furniture information\n",
    "    print(f\"Furniture object type: {type(furniture).__name__}\")\n",
    "    \n",
    "    # Check if furniture has any children\n",
    "    if hasattr(furniture, 'children') and furniture.children:\n",
    "        print(f\"Number of furniture children: {len(furniture.children)}\")\n",
    "        \n",
    "        # Inspect each furniture child\n",
    "        for i, child in enumerate(furniture.children):\n",
    "            print(f\"\\nFurniture Child #{i+1}:\")\n",
    "            child_type = type(child).__name__\n",
    "            print(f\"Type: {child_type}\")\n",
    "            \n",
    "            # Try to access common attributes\n",
    "            for attr in ['text', 'label', 'name', 'content_layer']:\n",
    "                if hasattr(child, attr):\n",
    "                    value = getattr(child, attr)\n",
    "                    print(f\"{attr}: {value}\")\n",
    "            \n",
    "            # Check for position information\n",
    "            if hasattr(child, 'prov') and child.prov:\n",
    "                page_numbers = [prov.page_no for prov in child.prov]\n",
    "                print(f\"Page Numbers: {page_numbers}\")\n",
    "                \n",
    "                # Get bounding box information if available\n",
    "                if hasattr(child.prov[0], 'bbox'):\n",
    "                    bbox = child.prov[0].bbox\n",
    "                    print(f\"Bounding Box: left={bbox.l}, top={bbox.t}, right={bbox.r}, bottom={bbox.b}\")\n",
    "            \n",
    "            # Show all other attributes\n",
    "            print(\"Other attributes:\")\n",
    "            for attr in dir(child):\n",
    "                if not attr.startswith('_') and attr not in ['text', 'label', 'name', 'content_layer', 'prov', 'children']:\n",
    "                    try:\n",
    "                        value = getattr(child, attr)\n",
    "                        if not callable(value) and not isinstance(value, (list, dict)) and str(value) != '':\n",
    "                            print(f\"  - {attr}: {value}\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    else:\n",
    "        print(\"Furniture has no children.\")\n",
    "    \n",
    "    # Show all furniture attributes\n",
    "    print(\"\\nFurniture attributes:\")\n",
    "    for attr in dir(furniture):\n",
    "        if not attr.startswith('_') and attr not in ['children']:\n",
    "            try:\n",
    "                value = getattr(furniture, attr)\n",
    "                if not callable(value) and not isinstance(value, (list, dict)) and str(value) != '':\n",
    "                    print(f\"- {attr}: {value}\")\n",
    "            except Exception as e:\n",
    "                print(f\"- {attr}: Error accessing ({str(e)})\")\n",
    "else:\n",
    "    print(\"Document does not have a furniture attribute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== UNDERSTANDING DOCLING CHUNKING ===\")\n",
    "\n",
    "# 1. Import the basic chunkers\n",
    "from docling.chunking import BaseChunker, HierarchicalChunker, HybridChunker\n",
    "\n",
    "# 2. Check what a chunker returns\n",
    "print(\"Chunkers in Docling return DocChunk objects that contain:\")\n",
    "print(\"- text: The actual text content of the chunk\")\n",
    "print(\"- meta: Metadata about the chunk (headings, page numbers, etc.)\")\n",
    "print(\"- Chunks are returned as iterators, so we typically convert to list\")\n",
    "\n",
    "# 3. Let's examine a single chunk to understand its structure\n",
    "chunker = HierarchicalChunker()  # Start with the hierarchical chunker\n",
    "chunks = list(chunker.chunk(document))\n",
    "\n",
    "if chunks:\n",
    "    # Examine the first chunk in detail\n",
    "    first_chunk = chunks[0]\n",
    "    print(\"\\nExample chunk structure:\")\n",
    "    print(f\"- Type: {type(first_chunk).__name__}\")\n",
    "    print(f\"- Text length: {len(first_chunk.text)} characters\")\n",
    "    print(f\"- Text preview: {first_chunk.text[:100]}...\")\n",
    "    \n",
    "    # Examine the metadata\n",
    "    print(\"\\nChunk metadata contains:\")\n",
    "    for attr in dir(first_chunk.meta):\n",
    "        if not attr.startswith('_') and not callable(getattr(first_chunk.meta, attr)):\n",
    "            value = getattr(first_chunk.meta, attr)\n",
    "            if isinstance(value, (list, dict)):\n",
    "                print(f\"- {attr}: {type(value).__name__} with {len(value)} items\")\n",
    "            else:\n",
    "                print(f\"- {attr}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different chunkers and their output\n",
    "print(\"\\n=== COMPARING DIFFERENT CHUNKERS ===\")\n",
    "\n",
    "# 1. HierarchicalChunker\n",
    "hierarchical_chunker = HierarchicalChunker()\n",
    "hierarchical_chunks = list(hierarchical_chunker.chunk(document))\n",
    "print(f\"HierarchicalChunker produced {len(hierarchical_chunks)} chunks\")\n",
    "\n",
    "# 2. HybridChunker with default settings\n",
    "hybrid_chunker = HybridChunker(tokenizer=tokenizer)\n",
    "hybrid_chunks = list(hybrid_chunker.chunk(document))\n",
    "print(f\"HybridChunker (default) produced {len(hybrid_chunks)} chunks\")\n",
    "\n",
    "# 3. HybridChunker with custom max_tokens\n",
    "hybrid_chunker_custom = HybridChunker(tokenizer=tokenizer, max_tokens=1000)\n",
    "hybrid_custom_chunks = list(hybrid_chunker_custom.chunk(document))\n",
    "print(f\"HybridChunker (max_tokens=1000) produced {len(hybrid_custom_chunks)} chunks\")\n",
    "\n",
    "# Print a summary of each chunker's output\n",
    "print(\"\\nChunker comparison summary:\")\n",
    "print(f\"1. HierarchicalChunker: {len(hierarchical_chunks)} chunks\")\n",
    "print(f\"   - First chunk length: {len(hierarchical_chunks[0].text)} chars\")\n",
    "print(f\"   - First chunk headings: {hierarchical_chunks[0].meta.headings}\")\n",
    "\n",
    "print(f\"\\n2. HybridChunker (default): {len(hybrid_chunks)} chunks\")\n",
    "print(f\"   - First chunk length: {len(hybrid_chunks[0].text)} chars\")\n",
    "print(f\"   - First chunk headings: {hybrid_chunks[0].meta.headings}\")\n",
    "\n",
    "print(f\"\\n3. HybridChunker (max_tokens=100): {len(hybrid_custom_chunks)} chunks\")\n",
    "print(f\"   - First chunk length: {len(hybrid_custom_chunks[0].text)} chars\")\n",
    "print(f\"   - First chunk headings: {hybrid_custom_chunks[0].meta.headings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate why HybridChunker is creating only one chunk\n",
    "print(\"\\n=== INVESTIGATING HYBRIDCHUNKER BEHAVIOR ===\")\n",
    "\n",
    "# 1. Check the total token count of the document\n",
    "full_text = \"\"\n",
    "for text_item in document.texts:\n",
    "    full_text += text_item.text + \" \"\n",
    "total_tokens = tokenizer.count_tokens(full_text)\n",
    "print(f\"Total document text: {len(full_text)} characters, {total_tokens} tokens\")\n",
    "\n",
    "# 2. Check if the document is small enough to fit in one chunk\n",
    "print(f\"Default max_tokens for HybridChunker: 2048\")\n",
    "print(f\"Is document smaller than default max_tokens? {total_tokens < 2048}\")\n",
    "\n",
    "# 3. Try with explicit parameters and debug output\n",
    "print(\"\\n=== TRYING HYBRIDCHUNKER WITH EXPLICIT PARAMETERS ===\")\n",
    "hybrid_chunker_debug = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens=100,  # Very small limit\n",
    "    merge_peers=False,  # Don't merge sections\n",
    "    min_chunk_chars=10,  # Allow very small chunks\n",
    "    min_chunk_size_ratio=0.1  # Allow chunks as small as 10% of max_tokens\n",
    ")\n",
    "\n",
    "# Get chunks and examine them\n",
    "debug_chunks = list(hybrid_chunker_debug.chunk(document))\n",
    "print(f\"HybridChunker with restrictive parameters produced {len(debug_chunks)} chunks\")\n",
    "\n",
    "# Check the token count of the single chunk if there's only one\n",
    "if len(debug_chunks) == 1:\n",
    "    chunk_tokens = tokenizer.count_tokens(debug_chunks[0].text)\n",
    "    print(f\"Single chunk token count: {chunk_tokens}\")\n",
    "    print(f\"Exceeds max_tokens limit of 100? {chunk_tokens > 100}\")\n",
    "    \n",
    "    # Check if the document has a structure that prevents splitting\n",
    "    print(\"\\nDocument structure analysis:\")\n",
    "    print(f\"Number of text elements: {len(document.texts)}\")\n",
    "    print(f\"Number of section headers: {sum(1 for item in document.texts if type(item).__name__ == 'SectionHeaderItem')}\")\n",
    "    \n",
    "    # Try to understand why it's not splitting\n",
    "    print(\"\\nPossible reasons for not splitting:\")\n",
    "    print(\"1. Document might have a structure that HybridChunker considers atomic\")\n",
    "    print(\"2. There might be a minimum chunk size that prevents splitting\")\n",
    "    print(\"3. The chunker might be configured to keep certain elements together\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the chunks produced by the restrictive HybridChunker\n",
    "print(\"\\n=== EXAMINING HYBRIDCHUNKER CHUNKS ===\")\n",
    "\n",
    "hybrid_chunker_detailed = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens=100,  # Small token limit\n",
    "    merge_peers=False,  # Don't merge sections\n",
    "    min_chunk_chars=10,  # Allow small chunks\n",
    "    min_chunk_size_ratio=0.1  # Allow chunks as small as 10% of max_tokens\n",
    ")\n",
    "\n",
    "detailed_chunks = list(hybrid_chunker_detailed.chunk(document))\n",
    "print(f\"Number of chunks: {len(detailed_chunks)}\")\n",
    "\n",
    "# Examine each chunk\n",
    "for i, chunk in enumerate(detailed_chunks):\n",
    "    token_count = tokenizer.count_tokens(chunk.text)\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"- Length: {len(chunk.text)} chars, {token_count} tokens\")\n",
    "    print(f\"- Headings: {chunk.meta.headings}\")\n",
    "    \n",
    "    # Get the first few words to understand the content\n",
    "    preview = chunk.text[:50] + \"...\" if len(chunk.text) > 50 else chunk.text\n",
    "    print(f\"- Preview: {preview}\")\n",
    "    \n",
    "    # Check if the chunk respects the token limit\n",
    "    if token_count > 100:\n",
    "        print(\"  WARNING: Chunk exceeds the max_tokens limit of 100\")\n",
    "        \n",
    "        # Try to understand why\n",
    "        doc_items = chunk.meta.doc_items if hasattr(chunk.meta, 'doc_items') else []\n",
    "        item_types = [type(item).__name__ for item in doc_items]\n",
    "        print(f\"  Contains item types: {set(item_types)}\")\n",
    "        \n",
    "        # Check if it's a single large text item that can't be split\n",
    "        if len(doc_items) == 1:\n",
    "            print(f\"  Single item chunk - cannot be split further\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
